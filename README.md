# Neural-Machine-Translation

**Neural Machine Translation using Transformer**

Implemented a Neural Machine Translation (NMT) model in Python using TensorFlow/Keras, focusing on English-to-French translation leveraging the Transformer architecture. The model integrates attention mechanisms for enhanced translation accuracy and handles sequence-to-sequence tasks effectively.

**Key Features:**

- Data Preprocessing: Cleaned and prepared English and French text data from the Anki dataset for model training.

- Model Architecture: Utilized Token and Position Embedding layers alongside Transformer Blocks with multi-head attention for capturing intricate linguistic patterns.

- Training and Validation: Trained the model using sparse categorical cross-entropy loss and Adam optimizer, achieving robust performance metrics through rigorous validation on train-test splits.

- Technologies: Python, TensorFlow/Keras, Tokenizers for text preprocessing, and TimeDistributed layers for sequential output generation.
